
### Step 1

- This baseline method is based off of the one found in [this](https://realpython.com/python-keras-text-classification/) tutorial.
- This uses the pre-trained glove.6B.50d model, which is stored locally to run. The current file path for this is "glove/glove.6B.50d.txt" which may need to be adjusted/downloaded to run on a different machine.
- Currently has the accuracy plot and model summary details displaying. This can be disabled by commenting out ```plt.show()``` and ```model.summary()``` respectively.
- This also uses the datasets from the same tutorial, which have been included in the ```data/``` folder. This means the model is running three times; one for each dataset.


This can be run with ```python cnn_base.py``` in the console.

### Step 2:

The dataset used for this is the PAN author profiling [dataset](https://pan.webis.de/clef19/pan19-web/author-profiling.html). Note: for the sake of time this model is only being tested on 10% of files in the pandata dataset. While a small percentage of the data, this should be be around ~70,000 tweets.

As the baseline method datasets only had two classes, some very slight changes had to made to run the baseline method on the new dataset:
  - Model is now using ```categorical_crossentropy``` as the loss function, due to there now being three classes (```binary_crossentropy``` would no longer make sense).
  - Output layer in model is now of size 3, reflective of this new loss function.
  - split test data into validation and test data (so model isnt learning via test)

<!-- ##### Performance (no val):
![performance_part_2](images/part2_performance_no_val_split.png)

<!-- ##### Performance (val):
![performance_part_2](images/part2_performance_val_split.png) -->

<!-- ##### Model:
![model_part_2](images/part2_model.png) --> -->



### Step 3:



#### Training GloVe on a custom corpus

One of the avenues I explored in improving the model was training my own GloVe model. This involves creating a text file of all words within the dataset. This can be generated by running ```python glove_prep.py``` in the console, which will generate a file called: ```full_pandata_corpus.txt```.

Next to train the model, the GloVe repository is needed:
- ```git clone https://github.com/stanfordnlp/GloVe.git``` in console

Next you need to modify the ```./demo.sh``` file in the GloVe repository.
- set CORPUS=full_pandata_corpus.txt
- change the line```if [ "$CORPUS" = 'text8' ]; then``` to ```if [ "$CORPUS" = 'full_pandata_corpus.txt' ]; then```
- remove the initial ```if``` script directly after ```make```

Finally run the script.
- ```cd GloVe```
- build: ```make``` in console
- and run: ```./demo.sh``` in console


#### Data pre-processing:
- https://link-springer-com.helicon.vuw.ac.nz/chapter/10.1007%2F978-981-15-5558-9_17
- Tweet Classification Using Deep Learning Approach to Predict Sensitive Personal Data



- https://link-springer-com.helicon.vuw.ac.nz/chapter/10.1007%2F978-3-030-60975-7_2
- NLTK package for stop words
We applied lower casing and removed punctuation, stopwords, urls and mentions

There wasn't any proper pre-processing done with the original model, so this was crucial to tackle.
I found two papers which used twitter data to see what pre-processing practices they used.

Paper 1: [Tweet Classification Using Deep Learning Approach to Predict Sensitive Personal Data](https://link.springer.com/chapter/10.1007%2F978-3-030-60975-7_2)
"""
steps applied:
- lower casing
- removed punctuation
- removed stopwords
- removed urls
- removed mentions.
- applied stemming

Paper 2: ***TODO: FIND PAPER***
![pre](images/paper-preprocessing.jpg)

Steps I took:

- converted text to lower case
- converted urls to "url"
- removed stop words
- removed punctuation
- replaced user names with "username"

Steps to check:
- remove numbers
- removal/emoji conversion
- stemming


#### Hyper-Parameter Tuning

One area I explored was auto tuning hyper paramters for the model. This was done by implementing a random search method which automatically tests the specified paramters. These values can be found and tweaked in the ```param_grid``` dictionary.

The best paramters found with random search are then stored and ran on the model. Possibly a redundant step to re-test as there model is essentially unchanged, but this allows me to get epoch by epoch output, exclusively from the best model.

Paramters currently being automatically tuned:
- Dense Layer Size
- Convolution Layer Kernel Size
- Convolution Layer Filter Size



#### Model Architecture:

- added dropout layer to help combat overfitting




##### Performance:
